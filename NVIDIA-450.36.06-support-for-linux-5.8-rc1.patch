From e0eea079f21b281116bed790a8afbf6f14bfad05 Mon Sep 17 00:00:00 2001
From: Xingyou Chen <rockrush@rockwork.org>
Date: Wed, 17 Jun 2020 16:27:31 +0800
Subject: [PATCH] [PATCH] port 450.36.06 to linux 5.8-rc1

Mainly changed mm_struct.mmap_sem into mm_struct.mmap_lock

Signed-off-by: Xingyou Chen <rockrush@rockwork.org>
---
 common/inc/nv-linux.h                       |   2 +-
 nvidia-drm/nvidia-drm-linux.c               |   4 +-
 nvidia-uvm/uvm8.c                           |  32 ++---
 nvidia-uvm/uvm8_api.h                       |   4 +-
 nvidia-uvm/uvm8_ats_faults.h                |   2 +-
 nvidia-uvm/uvm8_ats_ibm.c                   |   8 +-
 nvidia-uvm/uvm8_ats_ibm.h                   |   6 +-
 nvidia-uvm/uvm8_gpu_access_counters.c       |   4 +-
 nvidia-uvm/uvm8_gpu_non_replayable_faults.c |   4 +-
 nvidia-uvm/uvm8_gpu_replayable_faults.c     |   6 +-
 nvidia-uvm/uvm8_hmm.c                       |   2 +-
 nvidia-uvm/uvm8_hmm.h                       |   2 +-
 nvidia-uvm/uvm8_lock.c                      |   2 +-
 nvidia-uvm/uvm8_lock.h                      | 130 ++++++++++----------
 nvidia-uvm/uvm8_lock_test.c                 |   2 +-
 nvidia-uvm/uvm8_mem.c                       |   4 +-
 nvidia-uvm/uvm8_migrate.c                   |  18 +--
 nvidia-uvm/uvm8_migrate_pageable.c          |   8 +-
 nvidia-uvm/uvm8_migrate_pageable.h          |   2 +-
 nvidia-uvm/uvm8_policy.c                    |  22 ++--
 nvidia-uvm/uvm8_populate_pageable.c         |  18 +--
 nvidia-uvm/uvm8_populate_pageable.h         |   4 +-
 nvidia-uvm/uvm8_tools.c                     |   4 +-
 nvidia-uvm/uvm8_va_block.c                  |  22 ++--
 nvidia-uvm/uvm8_va_block.h                  |   8 +-
 nvidia-uvm/uvm8_va_block_types.h            |   2 +-
 nvidia-uvm/uvm8_va_range.c                  |   6 +-
 nvidia-uvm/uvm8_va_range.h                  |  12 +-
 nvidia-uvm/uvm8_va_space.c                  |  32 ++---
 nvidia-uvm/uvm8_va_space.h                  |   2 +-
 nvidia-uvm/uvm8_va_space_mm.c               |  10 +-
 nvidia-uvm/uvm8_va_space_mm.h               |   8 +-
 nvidia/nv-mmap.c                            |   2 +-
 nvidia/os-mlock.c                           |   8 +-
 34 files changed, 201 insertions(+), 201 deletions(-)

diff --git a/common/inc/nv-linux.h b/common/inc/nv-linux.h
index e20fa05..bf1f510 100644
--- a/common/inc/nv-linux.h
+++ b/common/inc/nv-linux.h
@@ -505,7 +505,7 @@ extern NvBool nvos_is_chipset_io_coherent(void);
 
 static inline void *nv_vmalloc(unsigned long size)
 {
-    void *ptr = __vmalloc(size, GFP_KERNEL, PAGE_KERNEL);
+    void *ptr = __vmalloc(size, GFP_KERNEL);
     if (ptr)
         NV_MEMDBG_ADD(ptr, size);
     return ptr;
diff --git a/nvidia-drm/nvidia-drm-linux.c b/nvidia-drm/nvidia-drm-linux.c
index 1d3e658..cfa37d7 100644
--- a/nvidia-drm/nvidia-drm-linux.c
+++ b/nvidia-drm/nvidia-drm-linux.c
@@ -103,11 +103,11 @@ int nv_drm_lock_user_pages(unsigned long address,
         return -ENOMEM;
     }
 
-    down_read(&mm->mmap_sem);
+    down_read(&mm->mmap_lock);
 
     pages_pinned = NV_GET_USER_PAGES(address, pages_count, write, force,
                                      user_pages, NULL);
-    up_read(&mm->mmap_sem);
+    up_read(&mm->mmap_lock);
 
     if (pages_pinned < 0 || (unsigned)pages_pinned < pages_count) {
         goto failed;
diff --git a/nvidia-uvm/uvm8.c b/nvidia-uvm/uvm8.c
index 447f72f..bdb8270 100644
--- a/nvidia-uvm/uvm8.c
+++ b/nvidia-uvm/uvm8.c
@@ -367,8 +367,8 @@ static void uvm_vm_open_managed(struct vm_area_struct *vma)
         return;
     }
 
-    // At this point we are guaranteed that the mmap_sem is held in write mode.
-    uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+    // At this point we are guaranteed that the mmap_lock is held in write mode.
+    uvm_record_lock_mmap_lock_write(&current->mm->mmap_lock);
 
     // Split vmas should always fall entirely within the old one, and be on one
     // side.
@@ -417,7 +417,7 @@ static void uvm_vm_open_managed(struct vm_area_struct *vma)
 
 out:
     uvm_va_space_up_write(va_space);
-    uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_write(&current->mm->mmap_lock);
 }
 
 static void uvm_vm_open_managed_entry(struct vm_area_struct *vma)
@@ -432,7 +432,7 @@ static void uvm_vm_close_managed(struct vm_area_struct *vma)
     bool make_zombie = false;
 
     if (current->mm != NULL)
-        uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_lock_mmap_lock_write(&current->mm->mmap_lock);
 
     UVM_ASSERT(uvm_va_space_initialized(va_space) == NV_OK);
 
@@ -460,7 +460,7 @@ static void uvm_vm_close_managed(struct vm_area_struct *vma)
         }
     }
 
-    // See uvm_mmap for why we need this in addition to mmap_sem
+    // See uvm_mmap for why we need this in addition to mmap_lock
     uvm_va_space_down_write(va_space);
 
     uvm_destroy_vma_managed(vma, make_zombie);
@@ -476,7 +476,7 @@ static void uvm_vm_close_managed(struct vm_area_struct *vma)
     uvm_va_space_up_write(va_space);
 
     if (current->mm != NULL)
-        uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_unlock_mmap_lock_write(&current->mm->mmap_lock);
 }
 
 static void uvm_vm_close_managed_entry(struct vm_area_struct *vma)
@@ -521,10 +521,10 @@ static vm_fault_t uvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 
     service_context->cpu_fault.wakeup_time_stamp = 0;
 
-    // The mmap_sem might be held in write mode, but the mode doesn't matter for
+    // The mmap_lock might be held in write mode, but the mode doesn't matter for
     // the purpose of lock ordering and we don't rely on it being in write
     // anywhere so just record it as read mode in all cases.
-    uvm_record_lock_mmap_sem_read(&vma->vm_mm->mmap_sem);
+    uvm_record_lock_mmap_lock_read(&vma->vm_mm->mmap_lock);
 
     do {
         bool do_sleep = false;
@@ -587,7 +587,7 @@ static vm_fault_t uvm_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
     }
 
     uvm_va_space_up_read(va_space);
-    uvm_record_unlock_mmap_sem_read(&vma->vm_mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_read(&vma->vm_mm->mmap_lock);
 
     if (status == NV_OK) {
         status = uvm_global_mask_check_ecc_error(&gpus_to_check_for_ecc);
@@ -663,7 +663,7 @@ static void uvm_vm_open_semaphore_pool(struct vm_area_struct *vma)
     bool is_fork = (vma->vm_mm != origin_vma->vm_mm);
     NV_STATUS status;
 
-    uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_lock_mmap_lock_write(&current->mm->mmap_lock);
 
     uvm_va_space_down_write(va_space);
 
@@ -703,7 +703,7 @@ static void uvm_vm_open_semaphore_pool(struct vm_area_struct *vma)
 
     uvm_va_space_up_write(va_space);
 
-    uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_write(&current->mm->mmap_lock);
 }
 
 static void uvm_vm_open_semaphore_pool_entry(struct vm_area_struct *vma)
@@ -718,7 +718,7 @@ static void uvm_vm_close_semaphore_pool(struct vm_area_struct *vma)
     uvm_va_space_t *va_space = uvm_va_space_get(vma->vm_file);
 
     if (current->mm != NULL)
-        uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_lock_mmap_lock_write(&current->mm->mmap_lock);
 
     uvm_va_space_down_read(va_space);
 
@@ -727,7 +727,7 @@ static void uvm_vm_close_semaphore_pool(struct vm_area_struct *vma)
     uvm_va_space_up_read(va_space);
 
     if (current->mm != NULL)
-        uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+        uvm_record_unlock_mmap_lock_write(&current->mm->mmap_lock);
 }
 
 static void uvm_vm_close_semaphore_pool_entry(struct vm_area_struct *vma)
@@ -797,7 +797,7 @@ static int uvm_mmap(struct file *filp, struct vm_area_struct *vma)
         return 0;
     }
 
-    uvm_record_lock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_lock_mmap_lock_write(&current->mm->mmap_lock);
 
     // VM_MIXEDMAP      Required to use vm_insert_page
     //
@@ -823,7 +823,7 @@ static int uvm_mmap(struct file *filp, struct vm_area_struct *vma)
     }
     vma_wrapper_allocated = true;
 
-    // The kernel has taken mmap_sem in write mode, but that doesn't prevent
+    // The kernel has taken mmap_lock in write mode, but that doesn't prevent
     // this va_space from being modified by the GPU fault path or from the ioctl
     // path where we don't have this mm for sure, so we have to lock the VA
     // space directly.
@@ -864,7 +864,7 @@ out:
     if (ret != 0 && vma_wrapper_allocated)
         uvm_vma_wrapper_destroy(vma->vm_private_data);
 
-    uvm_record_unlock_mmap_sem_write(&current->mm->mmap_sem);
+    uvm_record_unlock_mmap_lock_write(&current->mm->mmap_lock);
 
     uvm_up_read(&g_uvm_global.pm.lock);
 
diff --git a/nvidia-uvm/uvm8_api.h b/nvidia-uvm/uvm8_api.h
index 18e7189..993d8c5 100644
--- a/nvidia-uvm/uvm8_api.h
+++ b/nvidia-uvm/uvm8_api.h
@@ -200,7 +200,7 @@ static bool uvm_api_range_invalid_64k(NvU64 base, NvU64 length)
 // valid vmas. A vma is valid if the corresponding VM_SPECIAL flags are not
 // set.
 //
-// Locking: current->mm->mmap_sem must be locked
+// Locking: current->mm->mmap_lock must be locked
 bool uvm_is_valid_vma_range(NvU64 start, NvU64 length);
 
 // Check that the interval [base, base + length) is fully covered by UVM
@@ -208,7 +208,7 @@ bool uvm_is_valid_vma_range(NvU64 start, NvU64 length);
 // valid vmas (NV_WARN_NOTHING_TO_DO is returned). Any other input results on a
 // return status of NV_ERR_INVALID_ADDRESS
 //
-// Locking: current->mm->mmap_sem must be locked
+// Locking: current->mm->mmap_lock must be locked
 //          va_space->lock must be locked in shared or exclusive modes
 NV_STATUS uvm_api_range_type_check(uvm_va_space_t *va_space, NvU64 base, NvU64 length);
 
diff --git a/nvidia-uvm/uvm8_ats_faults.h b/nvidia-uvm/uvm8_ats_faults.h
index e001b60..0ac05bf 100644
--- a/nvidia-uvm/uvm8_ats_faults.h
+++ b/nvidia-uvm/uvm8_ats_faults.h
@@ -39,7 +39,7 @@ NV_STATUS uvm_ats_invalidate_tlbs(uvm_gpu_va_space_t *gpu_va_space,
 static bool uvm_can_ats_service_faults(uvm_gpu_va_space_t *gpu_va_space, struct mm_struct *mm)
 {
     if (mm)
-        uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+        uvm_assert_mmap_lock_locked(&mm->mmap_lock);
     if (gpu_va_space->ats.enabled)
         UVM_ASSERT(g_uvm_global.ats.enabled);
 
diff --git a/nvidia-uvm/uvm8_ats_ibm.c b/nvidia-uvm/uvm8_ats_ibm.c
index 93ede03..f2071f6 100644
--- a/nvidia-uvm/uvm8_ats_ibm.c
+++ b/nvidia-uvm/uvm8_ats_ibm.c
@@ -275,7 +275,7 @@ static NV_STATUS uvm_ats_ibm_register_gpu_va_space_kernel(uvm_gpu_va_space_t *gp
     if (current->mm != va_space->va_space_mm.mm)
         return NV_ERR_NOT_SUPPORTED;
 
-    uvm_assert_mmap_sem_locked_write(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked_write(&current->mm->mmap_lock);
 
     // pnv_npu2_init_context() doesn't handle being called multiple times for
     // the same GPU under the same mm, which could happen if multiple VA spaces
@@ -319,7 +319,7 @@ static void uvm_ats_ibm_unregister_gpu_va_space_kernel(uvm_gpu_va_space_t *gpu_v
     // pnv_npu2_destroy_context() may in turn call mmu_notifier_unregister(). If
     // uvm_va_space_mm_shutdown() is concurrently executing in another thread,
     // mmu_notifier_unregister() will wait for uvm_va_space_mm_shutdown() to
-    // finish. uvm_va_space_mm_shutdown() takes mmap_sem and the VA space lock,
+    // finish. uvm_va_space_mm_shutdown() takes mmap_lock and the VA space lock,
     // so we can't be holding those locks on this path.
     uvm_assert_unlocked_order(UVM_LOCK_ORDER_MMAP_SEM);
     uvm_assert_unlocked_order(UVM_LOCK_ORDER_VA_SPACE);
@@ -345,7 +345,7 @@ static void uvm_ats_ibm_register_gpu_va_space_driver(uvm_gpu_va_space_t *gpu_va_
         // tell the kernel to send TLB invalidations to the IOMMU. See kernel
         // commit 03b8abedf4f4965e7e9e0d4f92877c42c07ce19f for background.
         //
-        // This is safe to do without holding mm_users high or mmap_sem.
+        // This is safe to do without holding mm_users high or mmap_lock.
         if (bitmap_empty(va_space->ats.npu_active_mask, NV_MAX_NPUS))
             mm_context_add_copro(va_space->va_space_mm.mm);
 
@@ -468,7 +468,7 @@ NV_STATUS uvm_ats_ibm_service_fault(uvm_gpu_va_space_t *gpu_va_space,
     UVM_ASSERT(g_uvm_global.ats.enabled);
     UVM_ASSERT(gpu_va_space->ats.enabled);
     UVM_ASSERT(mm);
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&mm->mmap_lock);
 
     // TODO: Bug 2103669: Service more than a single fault at a time
     ret = NV_GET_USER_PAGES_REMOTE(NULL, mm, (unsigned long)fault_addr, 1, write, force, &page, NULL);
diff --git a/nvidia-uvm/uvm8_ats_ibm.h b/nvidia-uvm/uvm8_ats_ibm.h
index 528cbe3..71eb313 100644
--- a/nvidia-uvm/uvm8_ats_ibm.h
+++ b/nvidia-uvm/uvm8_ats_ibm.h
@@ -166,7 +166,7 @@ typedef struct
     // If UVM_ATS_IBM_SUPPORTED_IN_DRIVER() is 1 there are no such restrictions.
     //
     // LOCKING: The VA space lock must be held in write mode.
-    //          current->mm->mmap_sem must be held in write mode iff
+    //          current->mm->mmap_lock must be held in write mode iff
     //          UVM_ATS_IBM_SUPPORTED_IN_KERNEL() is 1.
     NV_STATUS uvm_ats_ibm_register_gpu_va_space(uvm_gpu_va_space_t *gpu_va_space);
 
@@ -175,13 +175,13 @@ typedef struct
     // accesses in this GPU VA space, and that no ATS fault handling for this
     // GPU will be attempted.
     //
-    // LOCKING: This function may block on mmap_sem and the VA space lock, so
+    // LOCKING: This function may block on mmap_lock and the VA space lock, so
     //          neither must be held.
     void uvm_ats_ibm_unregister_gpu_va_space(uvm_gpu_va_space_t *gpu_va_space);
 
     // Request the kernel to handle a fault.
     //
-    // LOCKING: mmap_sem must be held.
+    // LOCKING: mmap_lock must be held.
     NV_STATUS uvm_ats_ibm_service_fault(uvm_gpu_va_space_t *gpu_va_space,
                                         NvU64 fault_addr,
                                         uvm_fault_access_type_t access_type);
diff --git a/nvidia-uvm/uvm8_gpu_access_counters.c b/nvidia-uvm/uvm8_gpu_access_counters.c
index 8a57c23..1e22fc3 100644
--- a/nvidia-uvm/uvm8_gpu_access_counters.c
+++ b/nvidia-uvm/uvm8_gpu_access_counters.c
@@ -1174,7 +1174,7 @@ static NV_STATUS service_phys_single_va_block(uvm_gpu_t *gpu,
         // in order to lock it before locking the VA space.
         mm = uvm_va_space_mm_retain(va_space);
         if (mm)
-            uvm_down_read_mmap_sem(&mm->mmap_sem);
+            uvm_down_read_mmap_lock(&mm->mmap_lock);
 
         // Re-check that the VA block is valid after taking the VA space lock
         uvm_va_space_down_read(va_space);
@@ -1216,7 +1216,7 @@ done:
         uvm_va_space_up_read(va_space);
 
     if (mm) {
-        uvm_up_read_mmap_sem(&mm->mmap_sem);
+        uvm_up_read_mmap_lock(&mm->mmap_lock);
         uvm_va_space_mm_release(va_space);
     }
 
diff --git a/nvidia-uvm/uvm8_gpu_non_replayable_faults.c b/nvidia-uvm/uvm8_gpu_non_replayable_faults.c
index 0d3d305..b7f7e2c 100644
--- a/nvidia-uvm/uvm8_gpu_non_replayable_faults.c
+++ b/nvidia-uvm/uvm8_gpu_non_replayable_faults.c
@@ -508,7 +508,7 @@ static NV_STATUS service_fault(uvm_gpu_t *gpu, uvm_fault_buffer_entry_t *fault_e
     // can only service managed faults, not ATS/HMM faults.
     mm = uvm_va_space_mm_retain(va_space);
     if (mm)
-        uvm_down_read_mmap_sem(&mm->mmap_sem);
+        uvm_down_read_mmap_lock(&mm->mmap_lock);
 
     uvm_va_space_down_read(va_space);
 
@@ -559,7 +559,7 @@ static NV_STATUS service_fault(uvm_gpu_t *gpu, uvm_fault_buffer_entry_t *fault_e
 exit_no_channel:
     uvm_va_space_up_read(va_space);
     if (mm) {
-        uvm_up_read_mmap_sem(&mm->mmap_sem);
+        uvm_up_read_mmap_lock(&mm->mmap_lock);
         uvm_va_space_mm_release(va_space);
     }
 
diff --git a/nvidia-uvm/uvm8_gpu_replayable_faults.c b/nvidia-uvm/uvm8_gpu_replayable_faults.c
index d92e693..8d8b6e6 100644
--- a/nvidia-uvm/uvm8_gpu_replayable_faults.c
+++ b/nvidia-uvm/uvm8_gpu_replayable_faults.c
@@ -1463,7 +1463,7 @@ static NV_STATUS service_fault_batch(uvm_gpu_t *gpu,
 
                 uvm_va_space_up_read(va_space);
                 if (mm) {
-                    uvm_up_read_mmap_sem(&mm->mmap_sem);
+                    uvm_up_read_mmap_lock(&mm->mmap_lock);
                     uvm_va_space_mm_release(va_space);
                     mm = NULL;
                 }
@@ -1479,7 +1479,7 @@ static NV_STATUS service_fault_batch(uvm_gpu_t *gpu,
             // can only service managed faults, not ATS/HMM faults.
             mm = uvm_va_space_mm_retain(va_space);
             if (mm)
-                uvm_down_read_mmap_sem(&mm->mmap_sem);
+                uvm_down_read_mmap_lock(&mm->mmap_lock);
 
             uvm_va_space_down_read(va_space);
 
@@ -1586,7 +1586,7 @@ fail:
     if (va_space != NULL) {
         uvm_va_space_up_read(va_space);
         if (mm) {
-            uvm_up_read_mmap_sem(&mm->mmap_sem);
+            uvm_up_read_mmap_lock(&mm->mmap_lock);
             uvm_va_space_mm_release(va_space);
         }
     }
diff --git a/nvidia-uvm/uvm8_hmm.c b/nvidia-uvm/uvm8_hmm.c
index 67e81b6..bb3f0af 100644
--- a/nvidia-uvm/uvm8_hmm.c
+++ b/nvidia-uvm/uvm8_hmm.c
@@ -126,7 +126,7 @@ NV_STATUS uvm_hmm_mirror_register(uvm_va_space_t *va_space)
     if (!uvm_hmm_is_enabled(va_space))
         return NV_OK;
 
-    uvm_assert_mmap_sem_locked_write(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked_write(&current->mm->mmap_lock);
     uvm_assert_rwsem_locked_write(&va_space->lock);
 
     va_space->hmm_va_space.mirror.ops = &mirror_ops;
diff --git a/nvidia-uvm/uvm8_hmm.h b/nvidia-uvm/uvm8_hmm.h
index d2fc589..1fc3f22 100644
--- a/nvidia-uvm/uvm8_hmm.h
+++ b/nvidia-uvm/uvm8_hmm.h
@@ -57,7 +57,7 @@ typedef struct
 
     // Starts mirroring the HMM CPU table to the VA space.
     // Retains current->mm.
-    // Locking: mmap_sem must be held in write mode.
+    // Locking: mmap_lock must be held in write mode.
     //          va_space lock must be held in write mode.
     NV_STATUS uvm_hmm_mirror_register(uvm_va_space_t *va_space);
 
diff --git a/nvidia-uvm/uvm8_lock.c b/nvidia-uvm/uvm8_lock.c
index bf53eda..a3a9790 100644
--- a/nvidia-uvm/uvm8_lock.c
+++ b/nvidia-uvm/uvm8_lock.c
@@ -89,7 +89,7 @@ bool __uvm_record_lock(void *lock, uvm_lock_order_t lock_order, uvm_lock_flags_t
     //       these dependencies.
     if (lock_order == UVM_LOCK_ORDER_RM_GPUS) {
         if (test_bit(UVM_LOCK_ORDER_MMAP_SEM, uvm_context->acquired_lock_orders)) {
-            UVM_ERR_PRINT("Acquiring RM GPU lock with mmap_sem held\n");
+            UVM_ERR_PRINT("Acquiring RM GPU lock with mmap_lock held\n");
             correct = false;
         }
 
diff --git a/nvidia-uvm/uvm8_lock.h b/nvidia-uvm/uvm8_lock.h
index 32cfd47..97b5a62 100644
--- a/nvidia-uvm/uvm8_lock.h
+++ b/nvidia-uvm/uvm8_lock.h
@@ -44,7 +44,7 @@
 //      sleep cycles.
 //
 //      This lock is special: while it's taken by user-facing entry points,
-//      and may be taken before or after mmap_sem, this apparent violation of
+//      and may be taken before or after mmap_lock, this apparent violation of
 //      lock ordering is permissible because pm_lock may only be taken via
 //      trylock in read mode by paths which already hold any lower-level
 //      locks, as well as by paths subject to the kernel's freezer.  Paths
@@ -54,7 +54,7 @@
 //      infrequently, and only as part of to power management.  Starvation is
 //      not a concern.
 //
-//      The mmap_sem deadlock potential aside, the trylock approch is also
+//      The mmap_lock deadlock potential aside, the trylock approch is also
 //      motivated by the need to prevent user threads making UVM system calls
 //      from blocking when UVM is suspended: when the kernel suspends the
 //      system, the freezer employed to stop user tasks requires these tasks
@@ -96,29 +96,29 @@
 //        workqueue). Then the bottom-half releases the lock when that GPU's processing
 //        appears to be done.
 //
-// - mmap_sem
+// - mmap_lock
 //      Order: UVM_LOCK_ORDER_MMAP_SEM
 //      Reader/writer lock (rw_semaphore)
 //
-//      We're often called with the kernel already holding mmap_sem: mmap,
+//      We're often called with the kernel already holding mmap_lock: mmap,
 //      munmap, fault, etc. These operations may have to take any number of UVM
-//      locks, so mmap_sem requires special consideration in the lock order,
+//      locks, so mmap_lock requires special consideration in the lock order,
 //      since it's sometimes out of our control.
 //
-//      We need to hold mmap_sem when calling vm_insert_page, which means that
+//      We need to hold mmap_lock when calling vm_insert_page, which means that
 //      any time an operation (such as an ioctl) might need to install a CPU
-//      mapping, it must take current->mm->mmap_sem in read mode very early on.
+//      mapping, it must take current->mm->mmap_lock in read mode very early on.
 //
 //      However, current->mm is not necessarily the owning mm of the UVM vma.
 //      fork or fd passing via a UNIX doman socket can cause that. Notably, this
 //      is also the case when handling GPU faults from a kernel thread. This
-//      means we must lock current->mm->mmap_sem, then look up the UVM vma and
+//      means we must lock current->mm->mmap_lock, then look up the UVM vma and
 //      compare its mm before operating on that vma.
 //
-//      With HMM and ATS, the GPU fault handler takes mmap_sem. GPU faults may
+//      With HMM and ATS, the GPU fault handler takes mmap_lock. GPU faults may
 //      block forward progress of threads holding the RM GPUs lock until those
-//      faults are serviced, which means that mmap_sem cannot be held when the
-//      UVM driver calls into RM. In other words, mmap_sem and the RM GPUs lock
+//      faults are serviced, which means that mmap_lock cannot be held when the
+//      UVM driver calls into RM. In other words, mmap_lock and the RM GPUs lock
 //      are mutually exclusive.
 //
 // - Global VA spaces list lock
@@ -217,7 +217,7 @@
 //      Order: UVM_LOCK_ORDER_VA_SPACE
 //      Reader/writer lock (rw_semaphore) per uvm_va_space (UVM struct file)
 //
-//      This is the UVM equivalent of mmap_sem. It protects all state under that
+//      This is the UVM equivalent of mmap_lock. It protects all state under that
 //      va_space, such as the VA range tree.
 //
 //      Read mode: Faults (CPU and GPU), mapping creation, prefetches. These
@@ -450,29 +450,29 @@ bool __uvm_locking_initialized(void);
   // the given mode.
   #define uvm_check_locked(lock, flags) __uvm_check_locked((lock), (lock)->lock_order, (flags))
 
-  // Helpers for recording and asserting mmap_sem state
-  #define uvm_record_lock_mmap_sem_read(mmap_sem) \
-          uvm_record_lock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_SHARED)
+  // Helpers for recording and asserting mmap_lock state
+  #define uvm_record_lock_mmap_lock_read(mmap_lock) \
+          uvm_record_lock_raw((mmap_lock), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_SHARED)
 
-  #define uvm_record_unlock_mmap_sem_read(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_SHARED)
+  #define uvm_record_unlock_mmap_lock_read(mmap_lock) \
+          uvm_record_unlock_raw((mmap_lock), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_SHARED)
 
-  #define uvm_record_unlock_mmap_sem_read_out_of_order(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, \
+  #define uvm_record_unlock_mmap_lock_read_out_of_order(mmap_lock) \
+          uvm_record_unlock_raw((mmap_lock), UVM_LOCK_ORDER_MMAP_SEM, \
                                 UVM_LOCK_FLAGS_MODE_SHARED | UVM_LOCK_FLAGS_OUT_OF_ORDER)
 
-  #define uvm_record_lock_mmap_sem_write(mmap_sem) \
-          uvm_record_lock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
+  #define uvm_record_lock_mmap_lock_write(mmap_lock) \
+          uvm_record_lock_raw((mmap_lock), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
 
-  #define uvm_record_unlock_mmap_sem_write(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
+  #define uvm_record_unlock_mmap_lock_write(mmap_lock) \
+          uvm_record_unlock_raw((mmap_lock), UVM_LOCK_ORDER_MMAP_SEM, UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
 
-  #define uvm_record_unlock_mmap_sem_write_out_of_order(mmap_sem) \
-          uvm_record_unlock_raw((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, \
+  #define uvm_record_unlock_mmap_lock_write_out_of_order(mmap_lock) \
+          uvm_record_unlock_raw((mmap_lock), UVM_LOCK_ORDER_MMAP_SEM, \
                                 UVM_LOCK_FLAGS_MODE_EXCLUSIVE | UVM_LOCK_FLAGS_OUT_OF_ORDER)
 
-  #define uvm_check_locked_mmap_sem(mmap_sem, flags) \
-           __uvm_check_locked((mmap_sem), UVM_LOCK_ORDER_MMAP_SEM, (flags))
+  #define uvm_check_locked_mmap_lock(mmap_lock, flags) \
+           __uvm_check_locked((mmap_lock), UVM_LOCK_ORDER_MMAP_SEM, (flags))
 
   // Helpers for recording RM API lock usage around UVM-RM interfaces
   #define uvm_record_lock_rm_api() \
@@ -505,14 +505,14 @@ bool __uvm_locking_initialized(void);
       return false;
   }
 
-  #define uvm_record_lock_mmap_sem_read                 UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_read               UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_read_out_of_order  UVM_IGNORE_EXPR
-  #define uvm_record_lock_mmap_sem_write                UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_write              UVM_IGNORE_EXPR
-  #define uvm_record_unlock_mmap_sem_write_out_of_order UVM_IGNORE_EXPR
+  #define uvm_record_lock_mmap_lock_read                 UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_read               UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_read_out_of_order  UVM_IGNORE_EXPR
+  #define uvm_record_lock_mmap_lock_write                UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_write              UVM_IGNORE_EXPR
+  #define uvm_record_unlock_mmap_lock_write_out_of_order UVM_IGNORE_EXPR
 
-  #define uvm_check_locked_mmap_sem                     uvm_check_locked
+  #define uvm_check_locked_mmap_lock                     uvm_check_locked
 
   #define uvm_record_lock_rm_api()
   #define uvm_record_unlock_rm_api()
@@ -529,47 +529,47 @@ bool __uvm_locking_initialized(void);
 #define uvm_assert_lockable_order(order) UVM_ASSERT(__uvm_check_lockable_order(order, UVM_LOCK_FLAGS_MODE_ANY))
 #define uvm_assert_unlocked_order(order) UVM_ASSERT(__uvm_check_unlocked_order(order))
 
-// Helpers for locking mmap_sem and recording its usage
-#define uvm_assert_mmap_sem_locked_mode(mmap_sem, flags) ({                          \
-      typeof(mmap_sem) _sem = (mmap_sem);                                            \
-      UVM_ASSERT(rwsem_is_locked(_sem) && uvm_check_locked_mmap_sem(_sem, (flags))); \
+// Helpers for locking mmap_lock and recording its usage
+#define uvm_assert_mmap_lock_locked_mode(mmap_lock, flags) ({                          \
+      typeof(mmap_lock) _lock = (mmap_lock);                                            \
+      UVM_ASSERT(rwsem_is_locked(_lock) && uvm_check_locked_mmap_lock(_lock, (flags))); \
   })
 
-#define uvm_assert_mmap_sem_locked(mmap_sem) \
-        uvm_assert_mmap_sem_locked_mode((mmap_sem), UVM_LOCK_FLAGS_MODE_ANY)
-#define uvm_assert_mmap_sem_locked_read(mmap_sem) \
-        uvm_assert_mmap_sem_locked_mode((mmap_sem), UVM_LOCK_FLAGS_MODE_SHARED)
-#define uvm_assert_mmap_sem_locked_write(mmap_sem) \
-        uvm_assert_mmap_sem_locked_mode((mmap_sem), UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
-
-#define uvm_down_read_mmap_sem(mmap_sem) ({             \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        uvm_record_lock_mmap_sem_read(_sem);            \
-        down_read(_sem);                                \
+#define uvm_assert_mmap_lock_locked(mmap_lock) \
+        uvm_assert_mmap_lock_locked_mode((mmap_lock), UVM_LOCK_FLAGS_MODE_ANY)
+#define uvm_assert_mmap_lock_locked_read(mmap_lock) \
+        uvm_assert_mmap_lock_locked_mode((mmap_lock), UVM_LOCK_FLAGS_MODE_SHARED)
+#define uvm_assert_mmap_lock_locked_write(mmap_lock) \
+        uvm_assert_mmap_lock_locked_mode((mmap_lock), UVM_LOCK_FLAGS_MODE_EXCLUSIVE)
+
+#define uvm_down_read_mmap_lock(mmap_lock) ({             \
+        typeof(mmap_lock) _lock = (mmap_lock);             \
+        uvm_record_lock_mmap_lock_read(_lock);            \
+        down_read(_lock);                                \
     })
 
-#define uvm_up_read_mmap_sem(mmap_sem) ({               \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        up_read(_sem);                                  \
-        uvm_record_unlock_mmap_sem_read(_sem);          \
+#define uvm_up_read_mmap_lock(mmap_lock) ({               \
+        typeof(mmap_lock) _lock = (mmap_lock);             \
+        up_read(_lock);                                  \
+        uvm_record_unlock_mmap_lock_read(_lock);          \
     })
 
-#define uvm_up_read_mmap_sem_out_of_order(mmap_sem) ({      \
-        typeof(mmap_sem) _sem = (mmap_sem);                 \
-        up_read(_sem);                                      \
-        uvm_record_unlock_mmap_sem_read_out_of_order(_sem); \
+#define uvm_up_read_mmap_lock_out_of_order(mmap_lock) ({      \
+        typeof(mmap_lock) _lock = (mmap_lock);                 \
+        up_read(_lock);                                      \
+        uvm_record_unlock_mmap_lock_read_out_of_order(_lock); \
     })
 
-#define uvm_down_write_mmap_sem(mmap_sem) ({            \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        uvm_record_lock_mmap_sem_write(_sem);           \
-        down_write(_sem);                               \
+#define uvm_down_write_mmap_lock(mmap_lock) ({            \
+        typeof(mmap_lock) _lock = (mmap_lock);             \
+        uvm_record_lock_mmap_lock_write(_lock);           \
+        down_write(_lock);                               \
     })
 
-#define uvm_up_write_mmap_sem(mmap_sem) ({              \
-        typeof(mmap_sem) _sem = (mmap_sem);             \
-        up_write(_sem);                                 \
-        uvm_record_unlock_mmap_sem_write(_sem);         \
+#define uvm_up_write_mmap_lock(mmap_lock) ({              \
+        typeof(mmap_lock) _lock = (mmap_lock);             \
+        up_write(_lock);                                 \
+        uvm_record_unlock_mmap_lock_write(_lock);         \
     })
 
 // Helper for calling a UVM-RM interface function with lock recording
diff --git a/nvidia-uvm/uvm8_lock_test.c b/nvidia-uvm/uvm8_lock_test.c
index 583f0db..3c5b5e7 100644
--- a/nvidia-uvm/uvm8_lock_test.c
+++ b/nvidia-uvm/uvm8_lock_test.c
@@ -64,7 +64,7 @@ static bool fake_check_locked(uvm_lock_order_t lock_order, uvm_lock_flags_t flag
 
 // TODO: Bug 1799173: The lock asserts verify that the RM GPU lock isn't taken
 //       with the VA space lock in exclusive mode, and that the RM GPU lock
-//       isn't taken with mmap_sem held in any mode. Hack around this in the
+//       isn't taken with mmap_lock held in any mode. Hack around this in the
 //       test to enable the checks until we figure out something better.
 static bool skip_lock(uvm_lock_order_t lock_order, uvm_lock_flags_t flags)
 {
diff --git a/nvidia-uvm/uvm8_mem.c b/nvidia-uvm/uvm8_mem.c
index f88de44..1fa257e 100644
--- a/nvidia-uvm/uvm8_mem.c
+++ b/nvidia-uvm/uvm8_mem.c
@@ -482,7 +482,7 @@ static NV_STATUS uvm_mem_map_cpu_to_sysmem_user(uvm_mem_t *mem, struct vm_area_s
 
     UVM_ASSERT(uvm_mem_is_sysmem(mem));
     UVM_ASSERT(mem->is_user_allocation);
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&vma->vm_mm->mmap_lock);
 
     // TODO: Bug 1995015: high-order page allocations need to be allocated as
     // compound pages in order to be able to use vm_insert_page on them. This
@@ -506,7 +506,7 @@ static NV_STATUS uvm_mem_map_cpu_to_vidmem_user(uvm_mem_t *mem, struct vm_area_s
     size_t num_chunk_pages = mem->chunk_size / PAGE_SIZE;
 
     UVM_ASSERT(mem->is_user_allocation);
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&vma->vm_mm->mmap_lock);
     UVM_ASSERT(!uvm_mem_is_sysmem(mem));
     UVM_ASSERT(mem->backing_gpu != NULL);
     UVM_ASSERT(mem->backing_gpu->parent->numa_info.enabled);
diff --git a/nvidia-uvm/uvm8_migrate.c b/nvidia-uvm/uvm8_migrate.c
index 90a0fc8..44d3770 100644
--- a/nvidia-uvm/uvm8_migrate.c
+++ b/nvidia-uvm/uvm8_migrate.c
@@ -596,7 +596,7 @@ static NV_STATUS uvm_migrate(uvm_va_space_t *va_space,
     bool is_single_block;
     bool should_do_cpu_preunmap;
 
-    uvm_assert_mmap_sem_locked(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&current->mm->mmap_lock);
     uvm_assert_rwsem_locked(&va_space->lock);
 
     if (!first_va_range || first_va_range->type != UVM_VA_RANGE_TYPE_MANAGED)
@@ -854,8 +854,8 @@ NV_STATUS uvm_api_migrate(UVM_MIGRATE_PARAMS *params, struct file *filp)
         return NV_ERR_INVALID_ARGUMENT;
     }
 
-    // mmap_sem will be needed if we have to create CPU mappings
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    // mmap_lock will be needed if we have to create CPU mappings
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
     uvm_va_space_down_read(va_space);
 
     if (!is_async) {
@@ -930,14 +930,14 @@ NV_STATUS uvm_api_migrate(UVM_MIGRATE_PARAMS *params, struct file *filp)
     }
 
 done:
-    // We only need to hold mmap_sem to create new CPU mappings, so drop it if
+    // We only need to hold mmap_lock to create new CPU mappings, so drop it if
     // we need to wait for the tracker to finish.
     //
     // TODO: Bug 1766650: For large migrations with destination CPU, try
     //       benchmarks to see if a two-pass approach would be faster (first
     //       pass pushes all GPU work asynchronously, second pass updates CPU
     //       mappings synchronously).
-    uvm_up_read_mmap_sem_out_of_order(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock_out_of_order(&current->mm->mmap_lock);
 
     if (tracker_ptr) {
         if (params->semaphoreAddress && status == NV_OK) {
@@ -982,8 +982,8 @@ NV_STATUS uvm_api_migrate_range_group(UVM_MIGRATE_RANGE_GROUP_PARAMS *params, st
     NvU32 migrate_flags = 0;
     uvm_gpu_t *gpu = NULL;
 
-    // mmap_sem will be needed if we have to create CPU mappings
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    // mmap_lock will be needed if we have to create CPU mappings
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
     uvm_va_space_down_read(va_space);
 
     if (uvm_uuid_is_cpu(&params->destinationUuid)) {
@@ -1021,14 +1021,14 @@ NV_STATUS uvm_api_migrate_range_group(UVM_MIGRATE_RANGE_GROUP_PARAMS *params, st
     }
 
 done:
-    // We only need to hold mmap_sem to create new CPU mappings, so drop it if
+    // We only need to hold mmap_lock to create new CPU mappings, so drop it if
     // we need to wait for the tracker to finish.
     //
     // TODO: Bug 1766650: For large migrations with destination CPU, try
     //       benchmarks to see if a two-pass approach would be faster (first
     //       pass pushes all GPU work asynchronously, second pass updates CPU
     //       mappings synchronously).
-    uvm_up_read_mmap_sem_out_of_order(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock_out_of_order(&current->mm->mmap_lock);
 
     tracker_status = uvm_tracker_wait_deinit(&local_tracker);
     uvm_va_space_up_read(va_space);
diff --git a/nvidia-uvm/uvm8_migrate_pageable.c b/nvidia-uvm/uvm8_migrate_pageable.c
index dbb7b03..0790638 100644
--- a/nvidia-uvm/uvm8_migrate_pageable.c
+++ b/nvidia-uvm/uvm8_migrate_pageable.c
@@ -756,7 +756,7 @@ static NV_STATUS migrate_pageable_vma_region(struct vm_area_struct *vma,
     UVM_ASSERT(start >= vma->vm_start);
     UVM_ASSERT(outer <= vma->vm_end);
     UVM_ASSERT(outer - start <= UVM_MIGRATE_VMA_MAX_SIZE);
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&mm->mmap_lock);
     uvm_assert_rwsem_locked(&migrate_vma_state->va_space->lock);
 
     ret = migrate_vma(&g_migrate_vma_ops,
@@ -830,7 +830,7 @@ static NV_STATUS migrate_pageable_vma(struct vm_area_struct *vma,
     UVM_ASSERT(PAGE_ALIGNED(outer));
     UVM_ASSERT(vma->vm_end > start);
     UVM_ASSERT(vma->vm_start < outer);
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&mm->mmap_lock);
     uvm_assert_rwsem_locked(&va_space->lock);
 
     // Adjust to input range boundaries
@@ -882,7 +882,7 @@ static NV_STATUS migrate_pageable(struct mm_struct *mm,
 
     UVM_ASSERT(PAGE_ALIGNED(start));
     UVM_ASSERT(PAGE_ALIGNED(length));
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&mm->mmap_lock);
 
     vma = find_vma_intersection(mm, start, outer);
 
@@ -944,7 +944,7 @@ NV_STATUS uvm_migrate_pageable(uvm_va_space_t *va_space,
 
     UVM_ASSERT(PAGE_ALIGNED(start));
     UVM_ASSERT(PAGE_ALIGNED(length));
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&mm->mmap_lock);
 
     // We only check that dst_cpu_node_id is a valid node in the system and it
     // doesn't correspond to a GPU node. This is fine because alloc_pages_node
diff --git a/nvidia-uvm/uvm8_migrate_pageable.h b/nvidia-uvm/uvm8_migrate_pageable.h
index 10b8205..56147e1 100644
--- a/nvidia-uvm/uvm8_migrate_pageable.h
+++ b/nvidia-uvm/uvm8_migrate_pageable.h
@@ -54,7 +54,7 @@
 // NV_WARN_NOTHING_TO_DO to fall back to user space to complete the whole
 // migration, too.
 //
-// Locking: mmap_sem must be held in read or write mode
+// Locking: mmap_lock must be held in read or write mode
 NV_STATUS uvm_migrate_pageable(uvm_va_space_t *va_space,
                                struct mm_struct *mm,
                                const unsigned long start,
diff --git a/nvidia-uvm/uvm8_policy.c b/nvidia-uvm/uvm8_policy.c
index a912963..e94bb22 100644
--- a/nvidia-uvm/uvm8_policy.c
+++ b/nvidia-uvm/uvm8_policy.c
@@ -32,13 +32,13 @@
 // Returns true if the interval [start, start + length - 1] is entirely covered
 // by vmas.
 //
-// Locking: current->mm->mmap_sem must be locked
+// Locking: current->mm->mmap_lock must be locked
 bool uvm_is_valid_vma_range(NvU64 start, NvU64 length)
 {
     const NvU64 end = start + length;
     struct vm_area_struct *vma;
 
-    uvm_assert_mmap_sem_locked(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&current->mm->mmap_lock);
 
     vma = find_vma_intersection(current->mm, start, end);
 
@@ -233,7 +233,7 @@ NV_STATUS uvm_api_set_preferred_location(const UVM_SET_PREFERRED_LOCATION_PARAMS
 
     UVM_ASSERT(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
     uvm_va_space_down_write(va_space);
     has_va_space_write_lock = true;
 
@@ -306,7 +306,7 @@ done:
         uvm_va_space_up_write(va_space);
     else
         uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(&current->mm->mmap_lock);
 
     return status == NV_OK ? tracker_status : status;
 }
@@ -318,7 +318,7 @@ NV_STATUS uvm_api_unset_preferred_location(const UVM_UNSET_PREFERRED_LOCATION_PA
 
     UVM_ASSERT(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
     uvm_va_space_down_write(va_space);
 
     status = uvm_api_range_type_check(va_space, params->requestedBase, params->length);
@@ -329,7 +329,7 @@ NV_STATUS uvm_api_unset_preferred_location(const UVM_UNSET_PREFERRED_LOCATION_PA
         status = NV_OK;
 
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(&current->mm->mmap_lock);
     return status;
 }
 
@@ -410,7 +410,7 @@ static NV_STATUS accessed_by_set(uvm_va_space_t *va_space,
 
     UVM_ASSERT(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
     uvm_va_space_down_write(va_space);
 
     status = uvm_api_range_type_check(va_space, base, length);
@@ -474,7 +474,7 @@ static NV_STATUS accessed_by_set(uvm_va_space_t *va_space,
 
 done:
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(&current->mm->mmap_lock);
 
     return status;
 }
@@ -656,8 +656,8 @@ static NV_STATUS read_duplication_set(uvm_va_space_t *va_space, NvU64 base, NvU6
 
     UVM_ASSERT(va_space);
 
-    // We need mmap_sem as we may create CPU mappings
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    // We need mmap_lock as we may create CPU mappings
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
     uvm_va_space_down_write(va_space);
 
     status = uvm_api_range_type_check(va_space, base, length);
@@ -711,7 +711,7 @@ static NV_STATUS read_duplication_set(uvm_va_space_t *va_space, NvU64 base, NvU6
 
 done:
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(&current->mm->mmap_lock);
     return status;
 }
 
diff --git a/nvidia-uvm/uvm8_populate_pageable.c b/nvidia-uvm/uvm8_populate_pageable.c
index c47975e..4996a58 100644
--- a/nvidia-uvm/uvm8_populate_pageable.c
+++ b/nvidia-uvm/uvm8_populate_pageable.c
@@ -48,7 +48,7 @@ NV_STATUS uvm_populate_pageable_vma(struct vm_area_struct *vma,
     UVM_ASSERT(PAGE_ALIGNED(outer));
     UVM_ASSERT(vma->vm_end > start);
     UVM_ASSERT(vma->vm_start < outer);
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&mm->mmap_lock);
 
     if (!min_prot_ok)
         return NV_ERR_INVALID_ADDRESS;
@@ -61,17 +61,17 @@ NV_STATUS uvm_populate_pageable_vma(struct vm_area_struct *vma,
     vma_num_pages = vma_size / PAGE_SIZE;
 
     // If the input vma is managed by UVM, temporarily remove the record
-    // associated with the locking of mmap_sem, in order to avoid a "locked
-    // twice" validation error triggered when also acquiring mmap_sem in the
+    // associated with the locking of mmap_lock, in order to avoid a "locked
+    // twice" validation error triggered when also acquiring mmap_lock in the
     // page fault handler. The page fault is caused by get_user_pages.
     uvm_managed_vma = uvm_file_is_nvidia_uvm(vma->vm_file);
     if (uvm_managed_vma)
-        uvm_record_unlock_mmap_sem_read(&mm->mmap_sem);
+        uvm_record_unlock_mmap_lock_read(&mm->mmap_lock);
 
     ret = NV_GET_USER_PAGES(start, vma_num_pages, is_writable, 0, NULL, NULL);
 
     if (uvm_managed_vma)
-        uvm_record_lock_mmap_sem_read(&mm->mmap_sem);
+        uvm_record_lock_mmap_lock_read(&mm->mmap_lock);
 
     if (ret < 0)
         return errno_to_nv_status(ret);
@@ -94,7 +94,7 @@ NV_STATUS uvm_populate_pageable(struct mm_struct *mm,
 
     UVM_ASSERT(PAGE_ALIGNED(start));
     UVM_ASSERT(PAGE_ALIGNED(length));
-    uvm_assert_mmap_sem_locked(&mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&mm->mmap_lock);
 
     vma = find_vma_intersection(mm, start, outer);
 
@@ -153,16 +153,16 @@ NV_STATUS uvm_api_populate_pageable(const UVM_POPULATE_PAGEABLE_PARAMS *params,
     if (uvm_api_range_invalid(params->base, params->length))
         return NV_ERR_INVALID_ADDRESS;
 
-    // mmap_sem is needed to traverse the vmas in the input range and call into
+    // mmap_lock is needed to traverse the vmas in the input range and call into
     // get_user_pages
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
 
     if (allow_managed || uvm_va_space_range_empty(va_space, params->base, params->base + params->length - 1))
         status = uvm_populate_pageable(current->mm, params->base, params->length, min_prot);
     else
         status = NV_ERR_INVALID_ADDRESS;
 
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(&current->mm->mmap_lock);
 
     return status;
 }
diff --git a/nvidia-uvm/uvm8_populate_pageable.h b/nvidia-uvm/uvm8_populate_pageable.h
index 5ea3a95..b6509c9 100644
--- a/nvidia-uvm/uvm8_populate_pageable.h
+++ b/nvidia-uvm/uvm8_populate_pageable.h
@@ -28,7 +28,7 @@
 // [start:start+length) range. If any of the pages was not populated, we return
 // NV_ERR_NO_MEMORY.
 //
-// Locking: vma->vm_mm->mmap_sem must be held in read or write mode
+// Locking: vma->vm_mm->mmap_lock must be held in read or write mode
 NV_STATUS uvm_populate_pageable_vma(struct vm_area_struct *vma,
                                     unsigned long start,
                                     unsigned long length,
@@ -38,7 +38,7 @@ NV_STATUS uvm_populate_pageable_vma(struct vm_area_struct *vma,
 // range must be fully backed by vmas. If any of the pages was not populated,
 // we return NV_ERR_NO_MEMORY.
 //
-// Locking: mm->mmap_sem must be held in read or write mode
+// Locking: mm->mmap_lock must be held in read or write mode
 NV_STATUS uvm_populate_pageable(struct mm_struct *mm,
                                 unsigned long start,
                                 unsigned long length,
diff --git a/nvidia-uvm/uvm8_tools.c b/nvidia-uvm/uvm8_tools.c
index 0dae47e..778c955 100644
--- a/nvidia-uvm/uvm8_tools.c
+++ b/nvidia-uvm/uvm8_tools.c
@@ -264,9 +264,9 @@ static NV_STATUS map_user_pages(NvU64 user_va, NvU64 size, void **addr, struct p
         goto fail;
     }
 
-    down_read(&current->mm->mmap_sem);
+    down_read(&current->mm->mmap_lock);
     ret = NV_GET_USER_PAGES(user_va, num_pages, 1, 0, *pages, vmas);
-    up_read(&current->mm->mmap_sem);
+    up_read(&current->mm->mmap_lock);
     if (ret != num_pages) {
         status = NV_ERR_INVALID_ARGUMENT;
         goto fail;
diff --git a/nvidia-uvm/uvm8_va_block.c b/nvidia-uvm/uvm8_va_block.c
index 97e8780..b4df8f4 100644
--- a/nvidia-uvm/uvm8_va_block.c
+++ b/nvidia-uvm/uvm8_va_block.c
@@ -6463,7 +6463,7 @@ static NV_STATUS uvm_cpu_insert_page(struct vm_area_struct *vma,
 //  - Revoke mappings from other processors as appropriate so the CPU can map
 //    with new_prot permissions
 //  - Guarantee that vm_insert_page is safe to use (vma->vm_mm has a reference
-//    and mmap_sem is held in at least read mode)
+//    and mmap_lock is held in at least read mode)
 //  - Ensure that the struct page corresponding to the physical memory being
 //    mapped exists
 //  - Manage the block's residency bitmap
@@ -6528,7 +6528,7 @@ static NV_STATUS block_map_cpu_page_to(uvm_va_block_t *block,
     // us, so we can safely operate on the vma but we can't use
     // uvm_va_range_vma_current.
     vma = uvm_va_range_vma(va_range);
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&vma->vm_mm->mmap_lock);
     UVM_ASSERT(!uvm_va_space_mm_enabled(va_space) || va_space->va_space_mm.mm == vma->vm_mm);
 
     // Add the mapping
@@ -9927,7 +9927,7 @@ static NV_STATUS block_cpu_fault_locked(uvm_va_block_t *va_block,
     // 2) current->mm
     // 3) va_space->va_space_mm.mm
     //
-    // The kernel guarantees that vma->vm_mm has a reference taken with mmap_sem
+    // The kernel guarantees that vma->vm_mm has a reference taken with mmap_lock
     // held on the CPU fault path, so tell the fault handler to use that one.
     // current->mm might differ if we're on the access_process_vm (ptrace) path
     // or if another driver is calling get_user_pages.
@@ -10036,7 +10036,7 @@ NV_STATUS uvm_va_block_cpu_fault(uvm_va_block_t *va_block,
     // thinking they each owned the page.
     //
     // This function must only be called when it's safe to call vm_insert_page.
-    // That is, there's a reference held on the vma's vm_mm and vm_mm->mmap_sem
+    // That is, there's a reference held on the vma's vm_mm and vm_mm->mmap_lock
     // is held in at least read mode, but current->mm might not be vma->vm_mm.
     status = UVM_VA_BLOCK_LOCK_RETRY(va_block,
                                      &va_block_retry,
@@ -10673,9 +10673,9 @@ NV_STATUS uvm8_test_change_pte_mapping(UVM_TEST_CHANGE_PTE_MAPPING_PARAMS *param
 
     new_prot = g_uvm_test_pte_mapping_to_prot[params->mapping];
 
-    // mmap_sem isn't needed for invalidating CPU mappings, but it will be
+    // mmap_lock isn't needed for invalidating CPU mappings, but it will be
     // needed for inserting them.
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
     uvm_va_space_down_read(va_space);
 
     if (uvm_uuid_is_cpu(&params->uuid)) {
@@ -10758,7 +10758,7 @@ out_block:
 
 out:
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(&current->mm->mmap_lock);
 
     uvm_va_block_context_free(block_context);
 
@@ -10773,7 +10773,7 @@ NV_STATUS uvm8_test_va_block_info(UVM_TEST_VA_BLOCK_INFO_PARAMS *params, struct
 
     BUILD_BUG_ON(UVM_TEST_VA_BLOCK_SIZE != UVM_VA_BLOCK_SIZE);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
     uvm_va_space_down_read(va_space);
 
     status = uvm_va_block_find(va_space, params->lookup_address, &va_block);
@@ -10791,7 +10791,7 @@ NV_STATUS uvm8_test_va_block_info(UVM_TEST_VA_BLOCK_INFO_PARAMS *params, struct
 
 out:
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(&current->mm->mmap_lock);
     return status;
 }
 
@@ -10808,7 +10808,7 @@ NV_STATUS uvm8_test_va_residency_info(UVM_TEST_VA_RESIDENCY_INFO_PARAMS *params,
     unsigned release_block_count = 0;
     NvU64 addr = UVM_ALIGN_DOWN(params->lookup_address, PAGE_SIZE);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
     uvm_va_space_down_read(va_space);
 
     va_range = uvm_va_range_find(va_space, addr);
@@ -10951,7 +10951,7 @@ out:
             uvm_va_block_release(block);
     }
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(&current->mm->mmap_lock);
     return status;
 }
 
diff --git a/nvidia-uvm/uvm8_va_block.h b/nvidia-uvm/uvm8_va_block.h
index fd8ebdd..c38f146 100644
--- a/nvidia-uvm/uvm8_va_block.h
+++ b/nvidia-uvm/uvm8_va_block.h
@@ -963,8 +963,8 @@ NV_STATUS uvm_va_block_split(uvm_va_block_t *existing_va_block,
 // etc.).
 //
 // Locking:
-//  - vma->vm_mm->mmap_sem must be held in at least read mode. Note, that might
-//    not be the same as current->mm->mmap_sem.
+//  - vma->vm_mm->mmap_lock must be held in at least read mode. Note, that might
+//    not be the same as current->mm->mmap_lock.
 //  - va_space lock must be held in at least read mode
 //
 // service_context->block_context.mm is ignored and vma->vm_mm is used instead.
@@ -982,8 +982,8 @@ NV_STATUS uvm_va_block_cpu_fault(uvm_va_block_t *va_block,
 // context
 //
 // Locking:
-//  - vma->vm_mm->mmap_sem must be held in at least read mode. Note, that might
-//    not be the same as current->mm->mmap_sem. (only if
+//  - vma->vm_mm->mmap_lock must be held in at least read mode. Note, that might
+//    not be the same as current->mm->mmap_lock. (only if
 //    UVM_ID_IS_CPU(processor_id))
 //  - va_space lock must be held in at least read mode
 //  - va_block lock must be held
diff --git a/nvidia-uvm/uvm8_va_block_types.h b/nvidia-uvm/uvm8_va_block_types.h
index b0642dc..7e96fec 100644
--- a/nvidia-uvm/uvm8_va_block_types.h
+++ b/nvidia-uvm/uvm8_va_block_types.h
@@ -249,7 +249,7 @@ typedef struct
     //
     // 1) The mm will be valid (reference held) for the duration of the
     //    block operation.
-    // 2) mm->mmap_sem is held in at least read mode.
+    // 2) mm->mmap_lock is held in at least read mode.
     //
     // If this is NULL, the block operation skips anything which would require
     // the mm, such as creating CPU mappings.
diff --git a/nvidia-uvm/uvm8_va_range.c b/nvidia-uvm/uvm8_va_range.c
index c390027..bce7172 100644
--- a/nvidia-uvm/uvm8_va_range.c
+++ b/nvidia-uvm/uvm8_va_range.c
@@ -1634,7 +1634,7 @@ NV_STATUS uvm_va_range_check_logical_permissions(uvm_va_range_t *va_range,
         // GPU faults only check vma permissions if uvm_enable_builtin_tests is
         // set, because the Linux kernel can change vm_flags at any moment (for
         // example on mprotect) and here we are not guaranteed to have
-        // vma->vm_mm->mmap_sem. During tests we ensure that this scenario does
+        // vma->vm_mm->mmap_lock. During tests we ensure that this scenario does
         // not happen
         //
         // TODO: Bug 1896799: On HMM/ATS we could look up the mm here and do
@@ -1804,7 +1804,7 @@ NV_STATUS uvm8_test_va_range_info(UVM_TEST_VA_RANGE_INFO_PARAMS *params, struct
 
     va_space = uvm_va_space_get(filp);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
     uvm_va_space_down_read(va_space);
 
     va_range = uvm_va_range_find(va_space, params->lookup_address);
@@ -1865,7 +1865,7 @@ NV_STATUS uvm8_test_va_range_info(UVM_TEST_VA_RANGE_INFO_PARAMS *params, struct
 
 out:
     uvm_va_space_up_read(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(&current->mm->mmap_lock);
     return status;
 }
 
diff --git a/nvidia-uvm/uvm8_va_range.h b/nvidia-uvm/uvm8_va_range.h
index 81b17ec..18f5336 100644
--- a/nvidia-uvm/uvm8_va_range.h
+++ b/nvidia-uvm/uvm8_va_range.h
@@ -49,7 +49,7 @@
 // 1) RM allocations mapped to the GPU by UVM don't have associated UVM vmas
 //
 // 2) We don't always have a separate reference on the vma's mm_struct, so we
-//    can't always lock mmap_sem on paths where current->mm != vma->vm_mm.
+//    can't always lock mmap_lock on paths where current->mm != vma->vm_mm.
 //
 // 3) HMM vmas aren't ours, so we can't use their vm_private_data pointers.
 //
@@ -621,7 +621,7 @@ NV_STATUS uvm_va_space_split_span_as_needed(uvm_va_space_t *va_space,
                                             void *data);
 
 // Only call this if you're sure that either:
-// 1) You have a reference on the vma's vm_mm and that vma->vm_mm's mmap_sem is
+// 1) You have a reference on the vma's vm_mm and that vma->vm_mm's mmap_lock is
 //    held; or
 // 2) You won't be operating on the vma (as with vm_insert_page) or accessing
 //    any fields in the vma that can change without va_space->lock being held
@@ -671,7 +671,7 @@ static struct vm_area_struct *uvm_va_range_vma(uvm_va_range_t *va_range)
 
 // Check that the VA range's vma is safe to use under mm. If not, NULL is
 // returned. If the vma is returned, there must be a reference on mm and
-// mm->mmap_sem must be held.
+// mm->mmap_lock must be held.
 static struct vm_area_struct *uvm_va_range_vma_check(uvm_va_range_t *va_range, struct mm_struct *mm)
 {
     struct vm_area_struct *vma;
@@ -692,7 +692,7 @@ static struct vm_area_struct *uvm_va_range_vma_check(uvm_va_range_t *va_range, s
     //
     // Since the "safe" mm varies based on the path, we may not have a reference
     // on the vma's owning mm_struct. We won't know that until we look at the
-    // vma. By then it's too late to take mmap_sem since mmap_sem is above the
+    // vma. By then it's too late to take mmap_lock since mmap_lock is above the
     // va_space lock in our lock ordering, and we must be holding the va_space
     // lock to query the va_range. Hence the need to detect the cases in which
     // it's safe to operate on the vma.
@@ -701,7 +701,7 @@ static struct vm_area_struct *uvm_va_range_vma_check(uvm_va_range_t *va_range, s
     // operate on the vma at all. The vma can't be outright freed until we drop
     // the va_space lock so the pointer itself will remain valid, but its fields
     // (like vm_start and vm_end) could be modified behind our back. We also
-    // aren't allowed to call vm_insert_page unless we hold the vma's mmap_sem.
+    // aren't allowed to call vm_insert_page unless we hold the vma's mmap_lock.
     //
     // Note that if uvm_va_space_mm_enabled() is true, then vma->vm_mm must be
     // va_space->va_space_mm.mm because we enforce that at mmap.
@@ -712,7 +712,7 @@ static struct vm_area_struct *uvm_va_range_vma_check(uvm_va_range_t *va_range, s
     if (mm != vma->vm_mm)
         return NULL;
 
-    uvm_assert_mmap_sem_locked(&vma->vm_mm->mmap_sem);
+    uvm_assert_mmap_lock_locked(&vma->vm_mm->mmap_lock);
     return vma;
 }
 
diff --git a/nvidia-uvm/uvm8_va_space.c b/nvidia-uvm/uvm8_va_space.c
index 1c1f116..3600e6e 100644
--- a/nvidia-uvm/uvm8_va_space.c
+++ b/nvidia-uvm/uvm8_va_space.c
@@ -516,7 +516,7 @@ NV_STATUS uvm_va_space_initialize(uvm_va_space_t *va_space, NvU64 flags)
     if (flags & ~UVM_INIT_FLAGS_MASK)
         return NV_ERR_INVALID_ARGUMENT;
 
-    uvm_down_write_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_write_mmap_lock(&current->mm->mmap_lock);
     uvm_va_space_down_write(va_space);
 
     if (atomic_read(&va_space->initialized)) {
@@ -545,7 +545,7 @@ NV_STATUS uvm_va_space_initialize(uvm_va_space_t *va_space, NvU64 flags)
 
 out:
     uvm_va_space_up_write(va_space);
-    uvm_up_write_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_write_mmap_lock(&current->mm->mmap_lock);
 
     return status;
 }
@@ -833,9 +833,9 @@ NV_STATUS uvm_va_space_unregister_gpu(uvm_va_space_t *va_space, const NvProcesso
     if (gpu->parent->access_counters_supported)
         uvm_gpu_access_counters_disable(gpu, va_space);
 
-    // The mmap_sem lock is needed to establish CPU mappings to any pages
+    // The mmap_lock lock is needed to establish CPU mappings to any pages
     // evicted from the GPU if accessed by CPU is set for them.
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
 
     uvm_va_space_down_write(va_space);
 
@@ -852,7 +852,7 @@ NV_STATUS uvm_va_space_unregister_gpu(uvm_va_space_t *va_space, const NvProcesso
     uvm_processor_mask_clear(&va_space->gpu_unregister_in_progress, gpu->id);
 
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(&current->mm->mmap_lock);
 
     uvm_deferred_free_object_list(&deferred_free_list);
 
@@ -1112,7 +1112,7 @@ static void destroy_gpu_va_space(uvm_gpu_va_space_t *gpu_va_space)
     }
 
     // Note that this call may wait for faults to finish being serviced, which
-    // means it may depend on the VA space lock and mmap_sem.
+    // means it may depend on the VA space lock and mmap_lock.
     uvm_ats_ibm_unregister_gpu_va_space(gpu_va_space);
 
     uvm_gpu_va_space_release(gpu_va_space);
@@ -1280,14 +1280,14 @@ NV_STATUS uvm_va_space_register_gpu_va_space(uvm_va_space_t *va_space,
         return status;
     }
 
-    // uvm_ats_ibm_register_gpu_va_space() requires mmap_sem to be held in write
+    // uvm_ats_ibm_register_gpu_va_space() requires mmap_lock to be held in write
     // mode if ATS support is provided through the kernel. Otherwise we only
-    // need mmap_sem in read mode to handle potential CPU mapping changes in
+    // need mmap_lock in read mode to handle potential CPU mapping changes in
     // uvm_va_range_add_gpu_va_space().
     if (UVM_ATS_IBM_SUPPORTED_IN_KERNEL())
-        uvm_down_write_mmap_sem(&current->mm->mmap_sem);
+        uvm_down_write_mmap_lock(&current->mm->mmap_lock);
     else
-        uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+        uvm_down_read_mmap_lock(&current->mm->mmap_lock);
 
     uvm_va_space_down_write(va_space);
 
@@ -1342,9 +1342,9 @@ NV_STATUS uvm_va_space_register_gpu_va_space(uvm_va_space_t *va_space,
     uvm_va_space_up_write(va_space);
 
     if (UVM_ATS_IBM_SUPPORTED_IN_KERNEL())
-        uvm_up_write_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_write_mmap_lock(&current->mm->mmap_lock);
     else
-        uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_read_mmap_lock(&current->mm->mmap_lock);
 
     uvm_gpu_release(gpu);
     return NV_OK;
@@ -1363,9 +1363,9 @@ error:
     uvm_va_space_up_write(va_space);
 
     if (UVM_ATS_IBM_SUPPORTED_IN_KERNEL())
-        uvm_up_write_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_write_mmap_lock(&current->mm->mmap_lock);
     else
-        uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+        uvm_up_read_mmap_lock(&current->mm->mmap_lock);
 
     destroy_gpu_va_space(gpu_va_space);
 
@@ -1439,7 +1439,7 @@ NV_STATUS uvm_va_space_unregister_gpu_va_space(uvm_va_space_t *va_space, const N
     uvm_gpu_retain(gpu);
     uvm_va_space_up_read_rm(va_space);
 
-    uvm_down_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_down_read_mmap_lock(&current->mm->mmap_lock);
     uvm_va_space_down_write(va_space);
 
     // We dropped the lock so we have to re-verify that this gpu_va_space is
@@ -1457,7 +1457,7 @@ NV_STATUS uvm_va_space_unregister_gpu_va_space(uvm_va_space_t *va_space, const N
     }
 
     uvm_va_space_up_write(va_space);
-    uvm_up_read_mmap_sem(&current->mm->mmap_sem);
+    uvm_up_read_mmap_lock(&current->mm->mmap_lock);
 
     uvm_deferred_free_object_list(&deferred_free_list);
     uvm_gpu_va_space_release(gpu_va_space);
diff --git a/nvidia-uvm/uvm8_va_space.h b/nvidia-uvm/uvm8_va_space.h
index 6dadcce..927fba0 100644
--- a/nvidia-uvm/uvm8_va_space.h
+++ b/nvidia-uvm/uvm8_va_space.h
@@ -128,7 +128,7 @@ struct uvm_gpu_va_space_struct
     // registered in the VA space to avoid leaving stale entries of the VA range
     // that is going to be destroyed. Otherwise, these fault entries can be
     // attributed to new VA ranges reallocated at the same addresses. However,
-    // uvm_vm_close is called with mm->mmap_sem taken and we cannot take the ISR
+    // uvm_vm_close is called with mm->mmap_lock taken and we cannot take the ISR
     // lock. Therefore, we use a flag no notify the GPU fault handler that the
     // fault buffer needs to be flushed, before servicing the faults that belong
     // to the va_space.
diff --git a/nvidia-uvm/uvm8_va_space_mm.c b/nvidia-uvm/uvm8_va_space_mm.c
index 73988de..bc61ffa 100644
--- a/nvidia-uvm/uvm8_va_space_mm.c
+++ b/nvidia-uvm/uvm8_va_space_mm.c
@@ -246,7 +246,7 @@ bool uvm_va_space_mm_enabled(uvm_va_space_t *va_space)
     static int uvm_mmu_notifier_register(uvm_va_space_mm_t *va_space_mm)
     {
         UVM_ASSERT(va_space_mm->mm);
-        uvm_assert_mmap_sem_locked_write(&va_space_mm->mm->mmap_sem);
+        uvm_assert_mmap_lock_locked_write(&va_space_mm->mm->mmap_lock);
 
         if (UVM_ATS_IBM_SUPPORTED_IN_DRIVER() && g_uvm_global.ats.enabled)
             va_space_mm->mmu_notifier.ops = &uvm_mmu_notifier_ops_ats;
@@ -278,7 +278,7 @@ NV_STATUS uvm_va_space_mm_register(uvm_va_space_t *va_space)
     uvm_va_space_mm_t *va_space_mm = &va_space->va_space_mm;
     int ret;
 
-    uvm_assert_mmap_sem_locked_write(&current->mm->mmap_sem);
+    uvm_assert_mmap_lock_locked_write(&current->mm->mmap_lock);
     uvm_assert_rwsem_locked_write(&va_space->lock);
 
     UVM_ASSERT(uvm_va_space_initialized(va_space) != NV_OK);
@@ -309,7 +309,7 @@ void uvm_va_space_mm_unregister(uvm_va_space_t *va_space)
 {
     uvm_va_space_mm_t *va_space_mm = &va_space->va_space_mm;
 
-    // We can't hold the VA space lock or mmap_sem across this function since
+    // We can't hold the VA space lock or mmap_lock across this function since
     // mmu_notifier_unregister() may trigger uvm_va_space_mm_shutdown(), which
     // takes those locks and also waits for other threads which may take those
     // locks.
@@ -546,9 +546,9 @@ static NV_STATUS mm_read64(struct mm_struct *mm, NvU64 addr, NvU64 *val)
 
     UVM_ASSERT(IS_ALIGNED(addr, sizeof(val)));
 
-    uvm_down_read_mmap_sem(&mm->mmap_sem);
+    uvm_down_read_mmap_lock(&mm->mmap_lock);
     ret = NV_GET_USER_PAGES_REMOTE(NULL, mm, (unsigned long)addr, 1, write, force, &page, NULL);
-    uvm_up_read_mmap_sem(&mm->mmap_sem);
+    uvm_up_read_mmap_lock(&mm->mmap_lock);
 
     if (ret < 0)
         return errno_to_nv_status(ret);
diff --git a/nvidia-uvm/uvm8_va_space_mm.h b/nvidia-uvm/uvm8_va_space_mm.h
index 70aca34..7977e99 100644
--- a/nvidia-uvm/uvm8_va_space_mm.h
+++ b/nvidia-uvm/uvm8_va_space_mm.h
@@ -90,7 +90,7 @@ bool uvm_va_space_mm_enabled(uvm_va_space_t *va_space);
 //
 // Use uvm_va_space_mm_retain() to retrieve the mm.
 //
-// Locking: mmap_sem and the VA space lock must both be held for write.
+// Locking: mmap_lock and the VA space lock must both be held for write.
 NV_STATUS uvm_va_space_mm_register(uvm_va_space_t *va_space);
 
 // De-associate the mm from the va_space. This function won't return until all
@@ -98,18 +98,18 @@ NV_STATUS uvm_va_space_mm_register(uvm_va_space_t *va_space);
 // to uvm_va_space_mm_retain() will return NULL.
 //
 // This function may invoke uvm_va_space_mm_shutdown() so the caller must not
-// hold either mmap_sem or the VA space lock. Since this API must provide the
+// hold either mmap_lock or the VA space lock. Since this API must provide the
 // same guarantees as uvm_va_space_mm_shutdown(), the caller must also guarantee
 // prior to calling this function that all GPUs in this VA space have stopped
 // making accesses under this mm and will not be able to start again under that
 // VA space.
 //
-// Locking: This function may take both mmap_sem and the VA space lock.
+// Locking: This function may take both mmap_lock and the VA space lock.
 void uvm_va_space_mm_unregister(uvm_va_space_t *va_space);
 
 // Retains the current mm registered with this VA space. If no mm is currently
 // registered, NULL is returned. Otherwise, the returned mm will remain valid
-// for normal use (locking mmap_sem, find_vma, get_user_pages, etc) until
+// for normal use (locking mmap_lock, find_vma, get_user_pages, etc) until
 // uvm_va_space_mm_release() is called.
 //
 // It is NOT necessary to hold the VA space lock when calling this function.
diff --git a/nvidia/nv-mmap.c b/nvidia/nv-mmap.c
index c5987ab..798a160 100644
--- a/nvidia/nv-mmap.c
+++ b/nvidia/nv-mmap.c
@@ -225,7 +225,7 @@ static vm_fault_t nvidia_fault(
 
         /*
          * GPU wakeup cannot be completed directly in the fault handler due to the
-         * inability to take the GPU lock while mmap_sem is held.
+         * inability to take the GPU lock while mmap_lock is held.
          */
         status = rm_schedule_gpu_wakeup(nvl->sp[NV_DEV_STACK_GPU_WAKEUP], nv);
         if (status != NV_OK)
diff --git a/nvidia/os-mlock.c b/nvidia/os-mlock.c
index 1900fa1..3f411f4 100644
--- a/nvidia/os-mlock.c
+++ b/nvidia/os-mlock.c
@@ -43,7 +43,7 @@ NV_STATUS NV_API_CALL os_lookup_user_io_memory(
         return rmStatus;
     }
 
-    down_read(&mm->mmap_sem);
+    down_read(&mm->mmap_lock);
 
     vma = find_vma(mm, (NvUPtr)address);
     if ((vma == NULL) || ((vma->vm_flags & (VM_IO | VM_PFNMAP)) == 0))
@@ -76,7 +76,7 @@ NV_STATUS NV_API_CALL os_lookup_user_io_memory(
     }
 
 done:
-    up_read(&mm->mmap_sem);
+    up_read(&mm->mmap_lock);
 
     return rmStatus;
 }
@@ -110,10 +110,10 @@ NV_STATUS NV_API_CALL os_lock_user_pages(
         return rmStatus;
     }
 
-    down_read(&mm->mmap_sem);
+    down_read(&mm->mmap_lock);
     ret = NV_GET_USER_PAGES((unsigned long)address,
                             page_count, write, force, user_pages, NULL);
-    up_read(&mm->mmap_sem);
+    up_read(&mm->mmap_lock);
     pinned = ret;
 
     if (ret < 0)
-- 
2.27.0

